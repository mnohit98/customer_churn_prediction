{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction - Part 3: Model Evaluation\n",
    "\n",
    "## Overview\n",
    "This notebook covers:\n",
    "1. Detailed Model Evaluation\n",
    "2. Performance Metrics\n",
    "3. Visualization (ROC Curve, Confusion Matrix)\n",
    "4. Feature Importance Analysis\n",
    "5. Model Interpretation & Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve, auc,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Load models\n",
    "import joblib\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "X_test = pd.read_csv('data/X_test.csv')\n",
    "y_test = pd.read_csv('data/y_test.csv').squeeze()\n",
    "\n",
    "# Load scaler\n",
    "scaler = joblib.load('models/scaler.pkl')\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "# Load models\n",
    "lr_model = joblib.load('models/logistic_regression.pkl')\n",
    "rf_model = joblib.load('models/random_forest.pkl')\n",
    "\n",
    "# Try to load tuned models\n",
    "try:\n",
    "    rf_tuned_model = joblib.load('models/random_forest_tuned.pkl')\n",
    "    RF_TUNED_AVAILABLE = True\n",
    "except:\n",
    "    RF_TUNED_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    xgb_model = joblib.load('models/xgboost.pkl')\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "print(\"Data and models loaded successfully!\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name, scaled=False):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation function\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    \n",
    "    # Precision-Recall curve\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall_curve, precision_curve)\n",
    "    \n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'PR-AUC': pr_auc,\n",
    "        'Confusion Matrix': cm,\n",
    "        'FPR': fpr,\n",
    "        'TPR': tpr,\n",
    "        'Precision Curve': precision_curve,\n",
    "        'Recall Curve': recall_curve,\n",
    "        'Predictions': y_pred,\n",
    "        'Probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Evaluation function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Logistic Regression\n",
    "lr_results = evaluate_model(lr_model, X_test_scaled, y_test, 'Logistic Regression', scaled=True)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_results = evaluate_model(rf_model, X_test, y_test, 'Random Forest', scaled=False)\n",
    "\n",
    "# Evaluate Random Forest Tuned (if available)\n",
    "if RF_TUNED_AVAILABLE:\n",
    "    rf_tuned_results = evaluate_model(rf_tuned_model, X_test, y_test, 'Random Forest (Tuned)', scaled=False)\n",
    "\n",
    "# Evaluate XGBoost (if available)\n",
    "if XGBOOST_AVAILABLE:\n",
    "    xgb_results = evaluate_model(xgb_model, X_test, y_test, 'XGBoost', scaled=False)\n",
    "\n",
    "print(\"All models evaluated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Performance Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = {\n",
    "    'Model': [lr_results['Model'], rf_results['Model']],\n",
    "    'Accuracy': [lr_results['Accuracy'], rf_results['Accuracy']],\n",
    "    'Precision': [lr_results['Precision'], rf_results['Precision']],\n",
    "    'Recall': [lr_results['Recall'], rf_results['Recall']],\n",
    "    'F1-Score': [lr_results['F1-Score'], rf_results['F1-Score']],\n",
    "    'ROC-AUC': [lr_results['ROC-AUC'], rf_results['ROC-AUC']],\n",
    "    'PR-AUC': [lr_results['PR-AUC'], rf_results['PR-AUC']]\n",
    "}\n",
    "\n",
    "if RF_TUNED_AVAILABLE:\n",
    "    comparison_data['Model'].append(rf_tuned_results['Model'])\n",
    "    comparison_data['Accuracy'].append(rf_tuned_results['Accuracy'])\n",
    "    comparison_data['Precision'].append(rf_tuned_results['Precision'])\n",
    "    comparison_data['Recall'].append(rf_tuned_results['Recall'])\n",
    "    comparison_data['F1-Score'].append(rf_tuned_results['F1-Score'])\n",
    "    comparison_data['ROC-AUC'].append(rf_tuned_results['ROC-AUC'])\n",
    "    comparison_data['PR-AUC'].append(rf_tuned_results['PR-AUC'])\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    comparison_data['Model'].append(xgb_results['Model'])\n",
    "    comparison_data['Accuracy'].append(xgb_results['Accuracy'])\n",
    "    comparison_data['Precision'].append(xgb_results['Precision'])\n",
    "    comparison_data['Recall'].append(xgb_results['Recall'])\n",
    "    comparison_data['F1-Score'].append(xgb_results['F1-Score'])\n",
    "    comparison_data['ROC-AUC'].append(xgb_results['ROC-AUC'])\n",
    "    comparison_data['PR-AUC'].append(xgb_results['PR-AUC'])\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, model_name, ax):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False)\n",
    "    ax.set_title(f'{model_name}\\nConfusion Matrix', fontweight='bold')\n",
    "    ax.set_ylabel('True Label', fontsize=10)\n",
    "    ax.set_xlabel('Predicted Label', fontsize=10)\n",
    "    ax.set_xticklabels(['No Churn', 'Churn'])\n",
    "    ax.set_yticklabels(['No Churn', 'Churn'])\n",
    "\n",
    "num_models = 2 + int(RF_TUNED_AVAILABLE) + int(XGBOOST_AVAILABLE)\n",
    "fig, axes = plt.subplots(1, num_models, figsize=(6*num_models, 5))\n",
    "if num_models == 1: axes = [axes]\n",
    "\n",
    "idx = 0\n",
    "plot_confusion_matrix(lr_results['Confusion Matrix'], 'Logistic Regression', axes[idx])\n",
    "idx += 1\n",
    "plot_confusion_matrix(rf_results['Confusion Matrix'], 'Random Forest', axes[idx])\n",
    "\n",
    "if RF_TUNED_AVAILABLE:\n",
    "    idx += 1\n",
    "    plot_confusion_matrix(rf_tuned_results['Confusion Matrix'], 'Random Forest (Tuned)', axes[idx])\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    idx += 1\n",
    "    plot_confusion_matrix(xgb_results['Confusion Matrix'], 'XGBoost', axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_test.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "plt.title('Top 15 Feature Importance (Random Forest)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Evaluation Complete:\n",
    "1. All models evaluated with comprehensive metrics\n",
    "2. Visualizations created (ROC, PR curves, confusion matrices)\n",
    "3. Feature importance analyzed\n",
    "4. Business insights and recommendations generated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
