{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Customer Churn Prediction - Part 3: Model Evaluation\n",
    "\n",
    "## Overview\n",
    "This notebook covers:\n",
    "1. Detailed Model Evaluation\n",
    "2. Performance Metrics\n",
    "3. Visualization (ROC Curve, Confusion Matrix)\n",
    "4. Feature Importance Analysis\n",
    "5. Model Interpretation & Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve, auc,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Load models\n",
    "import joblib\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load test data\n",
    "X_test = pd.read_csv('data/X_test.csv')\n",
    "y_test = pd.read_csv('data/y_test.csv').squeeze()\n",
    "\n",
    "# Load scaler\n",
    "scaler = joblib.load('models/scaler.pkl')\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "# Load models\n",
    "lr_model = joblib.load('models/logistic_regression.pkl')\n",
    "rf_model = joblib.load('models/random_forest.pkl')\n",
    "\n",
    "# Try to load tuned models\n",
    "try:\n",
    "    rf_tuned_model = joblib.load('models/random_forest_tuned.pkl')\n",
    "    RF_TUNED_AVAILABLE = True\n",
    "except:\n",
    "    RF_TUNED_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    xgb_model = joblib.load('models/xgboost.pkl')\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "print(\"Data and models loaded successfully!\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Model Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name, scaled=False):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation function\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    if scaled:\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    \n",
    "    # Precision-Recall curve\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall_curve, precision_curve)\n",
    "    \n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'PR-AUC': pr_auc,\n",
    "        'Confusion Matrix': cm,\n",
    "        'FPR': fpr,\n",
    "        'TPR': tpr,\n",
    "        'Precision Curve': precision_curve,\n",
    "        'Recall Curve': recall_curve,\n",
    "        'Predictions': y_pred,\n",
    "        'Probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Evaluation function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Evaluate All Models"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Evaluate Logistic Regression\n",
    "lr_results = evaluate_model(lr_model, X_test_scaled, y_test, 'Logistic Regression', scaled=True)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_results = evaluate_model(rf_model, X_test, y_test, 'Random Forest', scaled=False)\n",
    "\n",
    "# Evaluate Random Forest Tuned (if available)\n",
    "if RF_TUNED_AVAILABLE:\n",
    "    rf_tuned_results = evaluate_model(rf_tuned_model, X_test, y_test, 'Random Forest (Tuned)', scaled=False)\n",
    "\n",
    "# Evaluate XGBoost (if available)\n",
    "if XGBOOST_AVAILABLE:\n",
    "    xgb_results = evaluate_model(xgb_model, X_test, y_test, 'XGBoost', scaled=False)\n",
    "\n",
    "print(\"All models evaluated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5: Performance Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = {\n",
    "    'Model': [lr_results['Model'], rf_results['Model']],\n",
    "    'Accuracy': [lr_results['Accuracy'], rf_results['Accuracy']],\n",
    "    'Precision': [lr_results['Precision'], rf_results['Precision']],\n",
    "    'Recall': [lr_results['Recall'], rf_results['Recall']],\n",
    "    'F1-Score': [lr_results['F1-Score'], rf_results['F1-Score']],\n",
    "    'ROC-AUC': [lr_results['ROC-AUC'], rf_results['ROC-AUC']],\n",
    "    'PR-AUC': [lr_results['PR-AUC'], rf_results['PR-AUC']]\n",
    "}\n",
    "\n",
    "if RF_TUNED_AVAILABLE:\n",
    "    comparison_data['Model'].append(rf_tuned_results['Model'])\n",
    "    comparison_data['Accuracy'].append(rf_tuned_results['Accuracy'])\n",
    "    comparison_data['Precision'].append(rf_tuned_results['Precision'])\n",
    "    comparison_data['Recall'].append(rf_tuned_results['Recall'])\n",
    "    comparison_data['F1-Score'].append(rf_tuned_results['F1-Score'])\n",
    "    comparison_data['ROC-AUC'].append(rf_tuned_results['ROC-AUC'])\n",
    "    comparison_data['PR-AUC'].append(rf_tuned_results['PR-AUC'])\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    comparison_data['Model'].append(xgb_results['Model'])\n",
    "    comparison_data['Accuracy'].append(xgb_results['Accuracy'])\n",
    "    comparison_data['Precision'].append(xgb_results['Precision'])\n",
    "    comparison_data['Recall'].append(xgb_results['Recall'])\n",
    "    comparison_data['F1-Score'].append(xgb_results['F1-Score'])\n",
    "    comparison_data['ROC-AUC'].append(xgb_results['ROC-AUC'])\n",
    "    comparison_data['PR-AUC'].append(xgb_results['PR-AUC'])\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Round for display\n",
    "comparison_df_rounded = comparison_df.round(4)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(comparison_df_rounded.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 6: Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_confusion_matrix(cm, model_name, ax):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False)\n",
    "    ax.set_title(f'{model_name}\\nConfusion Matrix', fontweight='bold')\n",
    "    ax.set_ylabel('True Label', fontsize=10)\n",
    "    ax.set_xlabel('Predicted Label', fontsize=10)\n",
    "    ax.set_xticklabels(['No Churn', 'Churn'])\n",
    "    ax.set_yticklabels(['No Churn', 'Churn'])\n",
    "\n",
    "# Create subplots\n",
    "num_models = 2\n",
    "if RF_TUNED_AVAILABLE:\n",
    "    num_models += 1\n",
    "if XGBOOST_AVAILABLE:\n",
    "    num_models += 1\n",
    "\n",
    "fig, axes = plt.subplots(1, num_models, figsize=(6*num_models, 5))\n",
    "if num_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "idx = 0\n",
    "plot_confusion_matrix(lr_results['Confusion Matrix'], 'Logistic Regression', axes[idx])\n",
    "idx += 1\n",
    "plot_confusion_matrix(rf_results['Confusion Matrix'], 'Random Forest', axes[idx])\n",
    "\n",
    "if RF_TUNED_AVAILABLE:\n",
    "    idx += 1\n",
    "    plot_confusion_matrix(rf_tuned_results['Confusion Matrix'], 'Random Forest (Tuned)', axes[idx])\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    idx += 1\n",
    "    plot_confusion_matrix(xgb_results['Confusion Matrix'], 'XGBoost', axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('models/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 7: ROC Curve Visualization"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot ROC curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.plot(lr_results['FPR'], lr_results['TPR'], \n",
    "         label=f\"Logistic Regression (AUC = {lr_results['ROC-AUC']:.4f})\", linewidth=2)\n",
    "plt.plot(rf_results['FPR'], rf_results['TPR'], \n",
    "         label=f\"Random Forest (AUC = {rf_results['ROC-AUC']:.4f})\", linewidth=2)\n",
    "\n",
    "if RF_TUNED_AVAILABLE:\n",
    "    plt.plot(rf_tuned_results['FPR'], rf_tuned_results['TPR'], \n",
    "             label=f\"Random Forest Tuned (AUC = {rf_tuned_results['ROC-AUC']:.4f})\", linewidth=2)\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    plt.plot(xgb_results['FPR'], xgb_results['TPR'], \n",
    "             label=f\"XGBoost (AUC = {xgb_results['ROC-AUC']:.4f})\", linewidth=2)\n",
    "\n",
    "# Diagonal line (random classifier)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.5000)', linewidth=1)\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('models/roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 8: Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot Precision-Recall curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.plot(lr_results['Recall Curve'], lr_results['Precision Curve'], \n",
    "         label=f\"Logistic Regression (AUC = {lr_results['PR-AUC']:.4f})\", linewidth=2)\n",
    "plt.plot(rf_results['Recall Curve'], rf_results['Precision Curve'], \n",
    "         label=f\"Random Forest (AUC = {rf_results['PR-AUC']:.4f})\", linewidth=2)\n",
    "\n",
    "if RF_TUNED_AVAILABLE:\n",
    "    plt.plot(rf_tuned_results['Recall Curve'], rf_tuned_results['Precision Curve'], \n",
    "             label=f\"Random Forest Tuned (AUC = {rf_tuned_results['PR-AUC']:.4f})\", linewidth=2)\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    plt.plot(xgb_results['Recall Curve'], xgb_results['Precision Curve'], \n",
    "             label=f\"XGBoost (AUC = {xgb_results['PR-AUC']:.4f})\", linewidth=2)\n",
    "\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curve Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower left', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('models/pr_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 9: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Get feature importance from Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_test.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 Most Important Features:\")\n",
    "print(\"=\"*50)\n",
    "print(feature_importance.head(15).to_string(index=False))\n",
    "\n",
    "# Visualize top features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['Importance'].values)\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'].values)\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.title('Top 15 Feature Importance (Random Forest)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('models/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 10: Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Detailed classification report for best model\n",
    "print(\"Classification Report - Random Forest:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, rf_results['Predictions'], \n",
    "                            target_names=['No Churn', 'Churn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 11: Model Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL INSIGHTS AND BUSINESS RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. KEY FINDINGS:\")\n",
    "print(\"   - Contract type is the strongest predictor of churn\")\n",
    "print(\"   - Tenure (customer loyalty) inversely related to churn\")\n",
    "print(\"   - Payment method affects churn probability\")\n",
    "print(\"   - Internet service type influences churn\")\n",
    "print(\"   - Monthly charges correlate with churn risk\")\n",
    "\n",
    "print(\"\\n2. BUSINESS RECOMMENDATIONS:\")\n",
    "print(\"   a) Target Month-to-month customers for retention campaigns\")\n",
    "print(\"   b) Offer incentives to long-tenure customers\")\n",
    "print(\"   c) Improve service quality for fiber optic internet users\")\n",
    "print(\"   d) Promote automatic payment methods to reduce churn\")\n",
    "print(\"   e) Create loyalty programs for customers with high tenure\")\n",
    "print(\"   f) Monitor customers with high monthly charges\")\n",
    "\n",
    "print(\"\\n3. MODEL PERFORMANCE:\")\n",
    "print(f\"   - Best Model: Random Forest\")\n",
    "print(f\"   - Accuracy: {rf_results['Accuracy']:.2%}\")\n",
    "print(f\"   - Precision: {rf_results['Precision']:.2%}\")\n",
    "print(f\"   - Recall: {rf_results['Recall']:.2%}\")\n",
    "print(f\"   - F1-Score: {rf_results['F1-Score']:.2%}\")\n",
    "print(f\"   - ROC-AUC: {rf_results['ROC-AUC']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "### Evaluation Complete:\n",
    "1. All models evaluated with comprehensive metrics\n",
    "2. Visualizations created (ROC, PR curves, confusion matrices)\n",
    "3. Feature importance analyzed\n",
    "4. Business insights and recommendations generated\n",
    "\n",
    "### Next Steps:\n",
    "- Use the trained model to predict churn for new customers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}